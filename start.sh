#!/bin/bash

# SENG550-A3 Quick Start Script
# This script helps you get the entire system up and running

echo "=== SENG550-A3 Data Engineering Pipeline Setup ==="
echo

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker is not running. Please start Docker first."
    exit 1
fi

echo "âœ… Docker is running"

# Navigate to project directory
cd "$(dirname "$0")"

echo "ğŸ“ Current directory: $(pwd)"

# Step 1: Start Docker environment
echo
echo "ğŸ³ Starting Docker environment..."
cd docker
docker-compose up -d

if [ $? -eq 0 ]; then
    echo "âœ… Docker containers started successfully"
else
    echo "âŒ Failed to start Docker containers"
    exit 1
fi

cd ..

# Wait for services to be ready
echo
echo "â³ Waiting for services to initialize..."
sleep 30

# Step 2: Check service health
echo
echo "ğŸ” Checking service health..."

# Check Spark Master
if curl -s http://localhost:8080 > /dev/null; then
    echo "âœ… Spark Master UI accessible at http://localhost:8080"
else
    echo "âš ï¸  Spark Master UI not yet ready at http://localhost:8080"
fi

# Check Airflow
if curl -s http://localhost:8081 > /dev/null; then
    echo "âœ… Airflow Web UI accessible at http://localhost:8081"
    echo "   Login: admin / admin"
else
    echo "âš ï¸  Airflow Web UI not yet ready at http://localhost:8081"
fi

# Check Redis
if docker exec redis redis-cli ping > /dev/null 2>&1; then
    echo "âœ… Redis is accessible"
else
    echo "âš ï¸  Redis not yet ready"
fi

echo
echo "ğŸ“‹ System Status Summary:"
echo "   - Spark Master: http://localhost:8080"
echo "   - Airflow Web UI: http://localhost:8081 (admin/admin)"
echo "   - Redis: localhost:6379"
echo
echo "ğŸš€ Next Steps:"
echo "   1. Wait 2-3 minutes for all services to fully initialize"
echo "   2. Access Airflow at http://localhost:8081"
echo "   3. Enable the DAGs:"
echo "      - incremental_processing_dag (runs every 4 seconds)"
echo "      - ml_training_dag (runs every 20 seconds)" 
echo "      - redis_cache_dag (runs every 20 seconds)"
echo "   4. Monitor the processing in Airflow UI"
echo
echo "ğŸ“– For detailed instructions, see README_IMPLEMENTATION.md"
echo

# Optional: Run initial data split if not done yet
if [ ! -f "part1/data/raw/0/orders_0.csv" ]; then
    echo "ğŸ“Š Running initial data split..."
    python part0_data_split.py
    if [ $? -eq 0 ]; then
        echo "âœ… Data split completed"
    else
        echo "âš ï¸  Data split failed - you may need to run it manually"
    fi
fi

echo "ğŸ‰ Setup complete! Your data engineering pipeline is ready."